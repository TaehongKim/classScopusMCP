#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¸í„°ë™í‹°ë¸Œ ì´ˆë¡ ê²€ìƒ‰ ì‹œìŠ¤í…œ
- ì‚¬ìš©ì í‚¤ì›Œë“œ ì…ë ¥
- Scopus ê²€ìƒ‰ + ë‹¤ì¤‘ ì†ŒìŠ¤ ì´ˆë¡ ê°€ì ¸ì˜¤ê¸°
- ê²°ê³¼ ì €ì¥ ë° ê´€ë¦¬
"""

import requests
import json
import time
import pandas as pd
import urllib3
import re
from typing import Dict, List, Optional
from urllib.parse import quote

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class InteractiveAbstractSearch:
    """ì¸í„°ë™í‹°ë¸Œ ì´ˆë¡ ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
    
    def __init__(self, scopus_api_key: str):
        self.scopus_api_key = scopus_api_key
        self.scopus_url = "https://api.elsevier.com/content/search/scopus"
        self.sources = {
            'crossref': 'https://api.crossref.org/works',
            'pubmed': 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils',
            'semantic_scholar': 'https://api.semanticscholar.org/v1',
            'openalex': 'https://api.openalex.org'
        }
        
        self.headers = {
            'Accept': 'application/json',
            'X-ELS-APIKey': scopus_api_key
        }
    
    def clean_abstract(self, abstract: str) -> str:
        """ì´ˆë¡ í…ìŠ¤íŠ¸ ì •ë¦¬ (HTML íƒœê·¸ ì œê±°, ê¸¸ì´ ì¡°ì •)"""
        if not abstract or abstract == 'N/A':
            return 'N/A'
        
        # HTML íƒœê·¸ ì œê±°
        clean_text = re.sub(r'<[^>]+>', '', abstract)
        
        # JATS íƒœê·¸ ì œê±°
        clean_text = re.sub(r'<jats:[^>]+>', '', clean_text)
        clean_text = re.sub(r'</jats:[^>]+>', '', clean_text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        clean_text = re.sub(r'\s+', ' ', clean_text)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        clean_text = clean_text.strip()
        
        # ê¸¸ì´ ì œí•œ (500ì)
        if len(clean_text) > 500:
            clean_text = clean_text[:500] + "..."
        
        return clean_text
    
    def search_scopus(self, query: str, count: int = 10, start: int = 0) -> Dict:
        """Scopus ê²€ìƒ‰"""
        
        params = {
            'query': query,
            'count': count,
            'start': start,
            'apiKey': self.scopus_api_key
        }
        
        try:
            response = requests.get(self.scopus_url, headers=self.headers, params=params, verify=False)
            
            if response.status_code == 200:
                return response.json()
            else:
                print(f"Scopus API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                return {}
                
        except Exception as e:
            print(f"Scopus API ì˜¤ë¥˜: {e}")
            return {}
    
    def get_abstract_from_crossref(self, doi: str) -> Dict:
        """Crossref APIë¡œ ì´ˆë¡ ê°€ì ¸ì˜¤ê¸°"""
        if not doi or doi == 'N/A':
            return {'source': 'crossref', 'abstract': 'N/A', 'success': False}
        
        url = f"{self.sources['crossref']}/{doi}"
        
        try:
            response = requests.get(url, verify=False, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if 'message' in data:
                    message = data['message']
                    abstract = message.get('abstract', 'N/A')
                    
                    if abstract and abstract != 'N/A':
                        clean_abstract = self.clean_abstract(abstract)
                        return {
                            'source': 'crossref',
                            'abstract': clean_abstract,
                            'success': True,
                            'title': message.get('title', ['N/A'])[0] if message.get('title') else 'N/A',
                            'quality_score': 9
                        }
            
            return {'source': 'crossref', 'abstract': 'N/A', 'success': False}
            
        except Exception as e:
            return {'source': 'crossref', 'abstract': 'N/A', 'success': False}
    
    def get_abstract_from_pubmed(self, doi: str) -> Dict:
        """PubMed APIë¡œ ì´ˆë¡ ê°€ì ¸ì˜¤ê¸°"""
        if not doi or doi == 'N/A':
            return {'source': 'pubmed', 'abstract': 'N/A', 'success': False}
        
        search_url = f"{self.sources['pubmed']}/esearch.fcgi"
        search_params = {
            'db': 'pubmed',
            'term': f"{doi}[doi]",
            'retmode': 'json'
        }
        
        try:
            response = requests.get(search_url, params=search_params, verify=False, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                if 'esearchresult' in data and 'idlist' in data['esearchresult']:
                    pmid_list = data['esearchresult']['idlist']
                    
                    if pmid_list:
                        pmid = pmid_list[0]
                        
                        fetch_url = f"{self.sources['pubmed']}/efetch.fcgi"
                        fetch_params = {
                            'db': 'pubmed',
                            'id': pmid,
                            'retmode': 'xml'
                        }
                        
                        fetch_response = requests.get(fetch_url, params=fetch_params, verify=False, timeout=10)
                        
                        if fetch_response.status_code == 200:
                            import xml.etree.ElementTree as ET
                            root = ET.fromstring(fetch_response.content)
                            
                            abstract = root.find('.//AbstractText')
                            if abstract is not None and abstract.text:
                                title = root.find('.//ArticleTitle')
                                title_text = title.text if title is not None else 'N/A'
                                
                                clean_abstract = self.clean_abstract(abstract.text)
                                return {
                                    'source': 'pubmed',
                                    'abstract': clean_abstract,
                                    'success': True,
                                    'title': title_text,
                                    'quality_score': 8
                                }
            
            return {'source': 'pubmed', 'abstract': 'N/A', 'success': False}
            
        except Exception as e:
            return {'source': 'pubmed', 'abstract': 'N/A', 'success': False}
    
    def get_abstract_from_semantic_scholar(self, doi: str) -> Dict:
        """Semantic Scholar APIë¡œ ì´ˆë¡ ê°€ì ¸ì˜¤ê¸°"""
        if not doi or doi == 'N/A':
            return {'source': 'semantic_scholar', 'abstract': 'N/A', 'success': False}
        
        url = f"{self.sources['semantic_scholar']}/paper/{doi}"
        
        try:
            response = requests.get(url, verify=False, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                abstract = data.get('abstract', 'N/A')
                if abstract and abstract != 'N/A':
                    clean_abstract = self.clean_abstract(abstract)
                    return {
                        'source': 'semantic_scholar',
                        'abstract': clean_abstract,
                        'success': True,
                        'title': data.get('title', 'N/A'),
                        'quality_score': 7
                    }
            
            return {'source': 'semantic_scholar', 'abstract': 'N/A', 'success': False}
            
        except Exception as e:
            return {'source': 'semantic_scholar', 'abstract': 'N/A', 'success': False}
    
    def get_best_abstract(self, doi: str) -> Dict:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ìµœê³  í’ˆì§ˆì˜ ì´ˆë¡ ê°€ì ¸ì˜¤ê¸°"""
        results = []
        
        sources = [
            ('crossref', self.get_abstract_from_crossref),
            ('pubmed', self.get_abstract_from_pubmed),
            ('semantic_scholar', self.get_abstract_from_semantic_scholar)
        ]
        
        for source_name, source_func in sources:
            result = source_func(doi)
            if result['success']:
                results.append(result)
            time.sleep(0.3)  # API í˜¸ì¶œ ê°„ê²©
        
        # í’ˆì§ˆ ì ìˆ˜ë¡œ ì •ë ¬í•˜ì—¬ ìµœê³  í’ˆì§ˆ ë°˜í™˜
        if results:
            best_result = max(results, key=lambda x: x.get('quality_score', 0))
            return best_result
        
        return {'source': 'none', 'abstract': 'N/A', 'success': False}
    
    def search_papers_with_abstracts(self, query: str, count: int = 10) -> List[Dict]:
        """í‚¤ì›Œë“œë¡œ ë…¼ë¬¸ ê²€ìƒ‰í•˜ê³  ì´ˆë¡ ê°€ì ¸ì˜¤ê¸°"""
        
        print(f"\nğŸ” í‚¤ì›Œë“œ '{query}'ë¡œ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘...")
        
        results = self.search_scopus(query=query, count=count, start=0)
        
        if results and 'search-results' in results:
            total_results = results['search-results'].get('opensearch:totalResults', 0)
            entries = results['search-results'].get('entry', [])
            
            print(f"âœ… ê²€ìƒ‰ ì„±ê³µ! ì´ {total_results}ê°œ ê²°ê³¼")
            print(f"í˜„ì¬ í˜ì´ì§€: {len(entries)}ê°œ ë…¼ë¬¸ ì²˜ë¦¬ ì¤‘...")
            print()
            
            papers = []
            for i, entry in enumerate(entries, 1):
                doi = entry.get('prism:doi', 'N/A')
                
                print(f"[{i}/{len(entries)}] {entry.get('dc:title', 'N/A')}")
                print(f"    ì €ì: {entry.get('dc:creator', 'N/A')}")
                print(f"    ì €ë„: {entry.get('prism:publicationName', 'N/A')}")
                print(f"    ë°œí–‰ì¼: {entry.get('prism:coverDate', 'N/A')}")
                print(f"    ì¸ìš©: {entry.get('citedby-count', 0)}")
                print(f"    DOI: {doi}")
                
                # ì´ˆë¡ ê°€ì ¸ì˜¤ê¸°
                if doi != 'N/A':
                    print("    ì´ˆë¡ ê²€ìƒ‰ ì¤‘...", end="")
                    abstract_result = self.get_best_abstract(doi)
                    
                    if abstract_result['success']:
                        print(f" âœ… ({abstract_result['source']})")
                        abstract = abstract_result['abstract']
                    else:
                        print(" âŒ")
                        abstract = 'N/A'
                else:
                    print("    DOI ì—†ìŒ - ì´ˆë¡ ê²€ìƒ‰ ë¶ˆê°€")
                    abstract = 'N/A'
                
                paper = {
                    'title': entry.get('dc:title', 'N/A'),
                    'authors': entry.get('dc:creator', 'N/A'),
                    'publication_name': entry.get('prism:publicationName', 'N/A'),
                    'publication_date': entry.get('prism:coverDate', 'N/A'),
                    'doi': doi,
                    'cited_by_count': entry.get('citedby-count', 0),
                    'scopus_id': entry.get('dc:identifier', '').replace('SCOPUS_ID:', ''),
                    'scopus_url': f"https://www.scopus.com/inward/record.uri?eid={entry.get('eid', '')}",
                    'abstract': abstract,
                    'abstract_source': abstract_result.get('source', 'N/A')
                }
                papers.append(paper)
                
                print("-" * 60)
                time.sleep(0.5)  # ì²˜ë¦¬ ê°„ê²©
            
            return papers
        else:
            print("âŒ ê²€ìƒ‰ ì‹¤íŒ¨")
            return []
    
    def save_to_csv(self, papers: List[Dict], filename: str = None):
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        
        if not papers:
            print("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if not filename:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"abstract_search_{timestamp}.csv"
        
        df_data = []
        for paper in papers:
            df_data.append({
                'Title': paper['title'],
                'Authors': paper['authors'],
                'Publication': paper['publication_name'],
                'Date': paper['publication_date'],
                'DOI': paper['doi'],
                'Citations': paper['cited_by_count'],
                'Scopus_ID': paper['scopus_id'],
                'Scopus_URL': paper['scopus_url'],
                'Abstract': paper['abstract'],
                'Abstract_Source': paper['abstract_source']
            })
        
        df = pd.DataFrame(df_data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ê²°ê³¼ê°€ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸ” ì¸í„°ë™í‹°ë¸Œ ì´ˆë¡ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print("íŠ¹ì§•:")
    print("1. ì‚¬ìš©ì í‚¤ì›Œë“œ ì…ë ¥")
    print("2. Scopus ê²€ìƒ‰ + ë‹¤ì¤‘ ì†ŒìŠ¤ ì´ˆë¡ ê°€ì ¸ì˜¤ê¸°")
    print("3. í’ˆì§ˆ ê¸°ë°˜ ì´ˆë¡ ì„ íƒ")
    print("4. CSV íŒŒì¼ë¡œ ê²°ê³¼ ì €ì¥")
    print("=" * 60)
    
    # API í‚¤ ì„¤ì •
    api_key = "920a284740d2c60fc3249e6e795e928c"
    client = InteractiveAbstractSearch(api_key)
    
    try:
        while True:
            print("\n" + "=" * 60)
            
            # í‚¤ì›Œë“œ ì…ë ¥
            query = input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit'): ").strip()
            
            if query.lower() == 'quit':
                print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not query:
                print("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì…ë ¥
            try:
                count = input("ê²€ìƒ‰í•  ë…¼ë¬¸ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸: 10): ").strip()
                count = int(count) if count else 10
                if count <= 0 or count > 50:
                    print("1-50 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ê¸°ë³¸ê°’ 10ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    count = 10
            except ValueError:
                print("ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ê¸°ë³¸ê°’ 10ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                count = 10
            
            # ë…¼ë¬¸ ê²€ìƒ‰ ë° ì´ˆë¡ ê°€ì ¸ì˜¤ê¸°
            papers = client.search_papers_with_abstracts(query, count)
            
            if papers:
                print(f"\nâœ… ê²€ìƒ‰ ì™„ë£Œ! {len(papers)}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                
                # ì´ˆë¡ì´ ìˆëŠ” ë…¼ë¬¸ ìˆ˜ ê³„ì‚°
                papers_with_abstract = sum(1 for paper in papers if paper['abstract'] != 'N/A')
                print(f"ì´ˆë¡ì´ ìˆëŠ” ë…¼ë¬¸: {papers_with_abstract}ê°œ")
                
                # ì €ì¥ ì—¬ë¶€ í™•ì¸
                save_choice = input("\nê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                if save_choice == 'y':
                    filename = input("íŒŒì¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸: ìë™ìƒì„±): ").strip()
                    if not filename:
                        timestamp = time.strftime("%Y%m%d_%H%M%S")
                        filename = f"abstract_search_{query.replace(' ', '_')}_{timestamp}.csv"
                    
                    client.save_to_csv(papers, filename)
                
                # ê³„ì† ê²€ìƒ‰í• ì§€ í™•ì¸
                continue_choice = input("\në‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                if continue_choice != 'y':
                    print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
            else:
                print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
                
    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main() 